{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fbe2ce-aefd-4a3b-852a-6223c6ff3aed",
   "metadata": {},
   "source": [
    "# Synthetizing textual information with embeddings\n",
    "\n",
    "Lino Galiana  \n",
    "2025-12-26\n",
    "\n",
    "> **Warning**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-warning callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Warning\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> Ce chapitre va évoluer prochainement.\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "<div class=\"badge-container\"><div class=\"badge-text\">If you want to try the examples in this tutorial:</div><a href=\"https://github.com/linogaliana/python-datascientist-notebooks/blob/main/notebooks/en/NLP/03_embedding.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/static/v1?logo=github&label=&message=View%20on%20GitHub&color=181717\" alt=\"View on GitHub\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/vscode-pytorch?autoLaunch=true&name=«03_embedding»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-vscode.sh»&init.personalInitArgs=«en/NLP%2003_embedding»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://custom-icon-badges.demolab.com/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=vsc&logoColor=white\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://datalab.sspcloud.fr/launcher/ide/jupyter-pytorch?autoLaunch=true&name=«03_embedding»&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fpython-datascientist%2Fmain%2Fsspcloud%2Finit-jupyter.sh»&init.personalInitArgs=«en/NLP%2003_embedding»\" target=\"_blank\" rel=\"noopener\"><img src=\"https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_Jupyter-orange?logo=Jupyter&logoColor=orange\" alt=\"Onyxia\"></a>\n",
    "<a href=\"https://colab.research.google.com/github/linogaliana/python-datascientist-notebooks-colab//en/blob/main//notebooks/en/NLP/03_embedding.ipynb\" target=\"_blank\" rel=\"noopener\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a><br></div>\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "This page builds on certain aspects presented in the [introductory section](../../content/NLP/02_exoclean.qmd).\n",
    "We will advance our understanding of NLP issues through language modeling.\n",
    "\n",
    "We start from the conclusion noted at the end of the previous chapter: frequentist approaches have several shortcomings, such as modeling language based on statistical regularities without considering word or phrase proximity, and difficulty incorporating context.\n",
    "\n",
    "The aim of this chapter is to address the first of those points. This will serve as an introduction to *embeddings*, the language representations at the core of modern language models used in everyday tools like `DeepL` or `ChatGPT`.\n",
    "\n",
    "## 1.1 Data Used\n",
    "\n",
    "We will continue our exploration of literature using the same three English-language authors:\n",
    "\n",
    "-   Edgar Allan Poe (EAP);\n",
    "-   HP Lovecraft (HPL);\n",
    "-   Mary Wollstonecraft Shelley (MWS).\n",
    "\n",
    "The dataset is available in a CSV file hosted on [`Github`](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/blob/master/data/spooky.csv), and can be directly downloaded from:\n",
    "<https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv>.\n",
    "\n",
    "To explore the topic of *embeddings*, we will use a language modeling task: predicting the author of a given text. A language model represents a text or language as a probability distribution over terms (usually words).\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Sources of Inspiration\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> This chapter is inspired by several online resources:\n",
    ">\n",
    "> -   A [first notebook on `Kaggle`](https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras)\n",
    ">     and [a second one](https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/notebook);\n",
    "> -   A [Github repository](https://github.com/GU4243-ADS/spring2018-project1-ginnyqg);\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "## 1.2 Required Packages\n",
    "\n",
    "As in the [previous section](../../content/NLP/02_exoclean.qmd), we need to install specialized NLP libraries along with their dependencies. This tutorial will use several libraries, including some that depend on `PyTorch`, which is a large framework.\n",
    "\n",
    "> **Important**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-important callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> PyTorch on SSPCloud\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> **The following note is only relevant for users of `SSPCloud`.**\n",
    ">\n",
    "> The standard `Python` services on `SSPCloud` (such as `vscode-python` and `jupyter-python`) do not include `PyTorch` by default. This library is quite large (around 600MB) and requires specific configuration to work seamlessly across different software environments. For ecological sustainability, this enhanced environment is not provided by default. However, when needed, an environment with `PyTorch` preinstalled is available.\n",
    ">\n",
    "> To access it, simply start a `vscode-pytorch` or `jupyter-pytorch` service. If you used one of the buttons above, this pre-configured service was automatically launched for you.\n",
    ">\n",
    "> </div>\n",
    "> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd64c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas spacy transformers scikit-learn langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c5419-66c1-4132-90c0-30b9519338ac",
   "metadata": {},
   "source": [
    "Next, since we will be using the `SpaCy` library with a corpus of English texts,\n",
    "we need to download the English NLP model. For this, you can refer to\n",
    "[the official `SpaCy` documentation](https://spacy.io/usage/models),\n",
    "which is extremely well-designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f18b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24daf953-1361-46d0-9f7b-9231281b8548",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "We will once again use the `spooky` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981dbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = 'https://github.com/GU4243-ADS/spring2018-project1-ginnyqg/raw/master/data/spooky.csv'\n",
    "spooky_df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de751692-d978-40f5-99e6-a27c2fece689",
   "metadata": {},
   "source": [
    "The dataset pairs each author with a sentence they wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced5628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spooky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a687f-7c63-4deb-a33b-7a2e7dfdc1fb",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing\n",
    "\n",
    "As discussed in the previous chapter, the first step in any work with textual data is often *preprocessing*, which typically includes tokenization and text cleaning.\n",
    "\n",
    "Here, we will stick to minimal preprocessing: removing punctuation and stop words (for visualization and count-based vectorization methods).\n",
    "\n",
    "To begin the cleaning process,\n",
    "we will use the `en_core_web_sm` model from `Spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae8ec6-66e0-4ea1-a6f1-d280b89749d1",
   "metadata": {},
   "source": [
    "We will use a `spacy` `pipe` that automates and parallelizes\n",
    "a number of operations. *Pipes* in NLP are similar to\n",
    "`scikit` pipelines or `pandas` pipes. They are well-suited tools\n",
    "for industrializing various *preprocessing* tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55323e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import spacy\n",
    "\n",
    "def clean_docs(\n",
    "    texts: List[str],\n",
    "    remove_stopwords: bool = False,\n",
    "    n_process: int = 4,\n",
    "    remove_punctuation: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Cleans a list of text documents by tokenizing, optionally removing stopwords, and optionally removing punctuation.\n",
    "\n",
    "    Parameters:\n",
    "        texts (List[str]): List of text documents to clean.\n",
    "        remove_stopwords (bool): Whether to remove stopwords. Default is False.\n",
    "        n_process (int): Number of processes to use for processing. Default is 4.\n",
    "        remove_punctuation (bool): Whether to remove punctuation. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of cleaned text documents.\n",
    "    \"\"\"\n",
    "    # Load spacy's nlp model\n",
    "    docs = nlp.pipe(\n",
    "        texts,\n",
    "        n_process=n_process,\n",
    "        disable=['parser', 'ner', 'lemmatizer', 'textcat']\n",
    "    )\n",
    "\n",
    "    # Pre-load stopwords for faster checking\n",
    "    stopwords = set(nlp.Defaults.stop_words)\n",
    "\n",
    "    # Process documents\n",
    "    docs_cleaned = (\n",
    "        ' '.join(\n",
    "            tok.text.lower().strip()\n",
    "            for tok in doc\n",
    "            if (not remove_punctuation or not tok.is_punct) and\n",
    "               (not remove_stopwords or tok.text.lower() not in stopwords)\n",
    "        )\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "    return list(docs_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb13c31-7ed3-4ed5-af3b-dcd21537d120",
   "metadata": {},
   "source": [
    "We apply the `clean_docs` function to our `pandas` column.\n",
    "Since `pandas.Series` are iterable, they behave like lists and\n",
    "work very well with our `spacy` pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_df['text_clean'] = clean_docs(spooky_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeab7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spooky_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c91ac-130e-489f-8ff2-11561380094d",
   "metadata": {},
   "source": [
    "## 2.2 Encoding the Target Variable\n",
    "\n",
    "We perform a simple encoding of the target variable:\n",
    "there are three categories (authors), represented by integers 0, 1, and 2.\n",
    "For this, we use `Scikit`’s `LabelEncoder`, previously introduced\n",
    "in the [modeling section](../../content/modelisation/0_preprocessing.qmd). We will use the `fit_transform` method, which conveniently combines\n",
    "fitting (i.e., creating a mapping between numerical values and labels)\n",
    "and transforming the same column in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "spooky_df['author_encoded'] = le.fit_transform(spooky_df['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6542a0d-440f-4c73-9e5f-bf4f77ee2bd6",
   "metadata": {},
   "source": [
    "We can check the classes of our `LabelEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea2e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP', 'HPL', 'MWS'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccaf7c-6600-493c-9e93-abc1994aad25",
   "metadata": {},
   "source": [
    "## 2.3 Creating the Training and Test Sets\n",
    "\n",
    "We set aside a test sample (20%) before performing any analysis (even descriptive).\n",
    "This ensures a rigorous evaluation of our models at the end, since these data will never have been seen during training.\n",
    "\n",
    "Our initial dataset is not balanced—some authors have more texts than others.\n",
    "To ensure fair evaluation of our model, we will stratify the sampling so that the training and test sets contain a similar distribution of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = spooky_df[\"author\"]\n",
    "X = spooky_df['text_clean']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a231679-470f-4b38-9302-d2a67e393f18",
   "metadata": {},
   "source": [
    "Preview of the first element in `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60305d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this process however afforded me no means of ascertaining the dimensions of my dungeon as i might make its circuit and return to the point whence i set out without being aware of the fact so perfectly uniform seemed the wall'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1936fc-e181-4bf0-84ec-dce14ca85bc1",
   "metadata": {},
   "source": [
    "# 3. Vectorization Using the *Bag of Words* Approach\n",
    "\n",
    "Representing our texts as a bag of words allows us to vectorize the corpus and thus obtain a numerical representation of each text. From there, we can perform various types of modeling tasks.\n",
    "\n",
    "Let’s define our vector representation using TF-IDF with `Scikit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c4183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "])\n",
    "pipeline_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358afad-a81c-4830-9702-00aaf6e5ec9a",
   "metadata": {},
   "source": [
    "Let’s go ahead and train our model to vectorize the text using the TF-IDF method. At this stage, we are not yet concerned with evaluation, so we will train on the entire dataset, not just `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-2');</script></body>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_tfidf.fit(spooky_df['text_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9c463-bda0-4326-8717-b8bd64883168",
   "metadata": {},
   "source": [
    "## 3.1 Finding the Most Similar Text\n",
    "\n",
    "First, we can look for the text that is closest—according to TF-IDF similarity—to a given sentence. Let’s take the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"He was afraid by Frankenstein monster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f92ec-d02a-4c31-93b3-3163564f793d",
   "metadata": {},
   "source": [
    "How can we find the text most similar to this one? We need to transform our sentence into the same vector representation, then compare it to the other texts using that same form.\n",
    "\n",
    "This is essentially an information retrieval task—a classic NLP use case—implemented, for example, by search engines. Since the term “Frankenstein” is quite distinctive, we should be able to identify similarities with other texts written by Mary Shelley using TF-IDF.\n",
    "\n",
    "A metric commonly used to compare vectors is cosine similarity. This is a central measure in modern NLP. While it is more meaningful with dense vectors (which we’ll explore soon), it still provides a useful exercise for understanding similarity between two vectors, even when those vectors are sparse, as in the bag-of-words approach.\n",
    "\n",
    "If each dimension of a vector represents a direction, cosine similarity measures the angle between two vectors. The smaller the angle, the closer the vectors.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png)\n",
    "\n",
    "### 3.1.1 With `Scikit-Learn`\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Exercise 1: Similarity Search with TF-IDF\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> 1.  Use the `transform` method to vectorize the entire training corpus.\n",
    ">\n",
    "> 2.  Assuming your vectorized training set is named `X_train_tfidf`, you can convert it to a *DataFrame* with the following command:\n",
    ">\n",
    "> ``` python\n",
    "> X_train_tfidf = pd.DataFrame(\n",
    ">     X_train_tfidf.todense(), columns=pipeline_tfidf.get_feature_names_out()\n",
    "> )\n",
    "> ```\n",
    ">\n",
    "> 1.  Use `Scikit`’s `cosine_similarity` method to compute cosine similarity between your vectorized text and the training corpus using this code:\n",
    ">\n",
    "> ``` python\n",
    "> import numpy as np\n",
    "> from sklearn.metrics.pairwise import cosine_similarity\n",
    ">\n",
    "> cosine_similarities = cosine_similarity(\n",
    ">     X_train_tfidf,\n",
    ">     pipeline_tfidf.transform([text])\n",
    "> ).flatten()\n",
    ">\n",
    "> top_4_indices = np.argsort(cosine_similarities)[-4:][::-1]  # Descending sort\n",
    "> top_4_similarities = cosine_similarities[top_4_indices]\n",
    "> ```\n",
    ">\n",
    "> 1.  Retrieve the corresponding documents. Are you satisfied with the result? Do you understand what happened?\n",
    ">\n",
    "> </div>\n",
    "> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exo1-q1-q2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = (\n",
    "    pipeline_tfidf.transform(spooky_df['text_clean'])\n",
    ")\n",
    "X_train_tfidf=pd.DataFrame(\n",
    "    X_train_tfidf.todense(),columns=pipeline_tfidf.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exo1-q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    X_train_tfidf,\n",
    "    pipeline_tfidf.transform([text])\n",
    ").flatten()\n",
    "\n",
    "top_4_indices = np.argsort(cosine_similarities)[-4:][::-1]  # Sort and reverse for descending order\n",
    "top_4_similarities = cosine_similarities[top_4_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53226-115d-4a7f-8dcb-7a87af464002",
   "metadata": {},
   "source": [
    "A l’issue de l’exercice, les 4 textes les plus similaires sont:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44fe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_plus_proches = spooky_df.iloc[top_4_indices].loc[:, [\"text\", \"author\"]]\n",
    "documents_plus_proches['score'] = top_4_similarities\n",
    "\n",
    "documents_plus_proches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b34583-9a5e-4801-8f3f-6af61370c9db",
   "metadata": {},
   "source": [
    "### 3.1.2 With `Langchain`\n",
    "\n",
    "This approach to computing text similarity is rather tedious with `Scikit`. With the rapid development of `Python` applications leveraging language models, a rich ecosystem has emerged to make these tasks achievable in just a few lines of code.\n",
    "\n",
    "Among the most valuable tools is [`Langchain`](https://www.langchain.com/), a high-level `Python` ecosystem for building production-ready pipelines using textual data.\n",
    "\n",
    "We will proceed here in two steps:\n",
    "\n",
    "-   Create a *retriever*, which involves vectorizing our corpus (texts from the three authors) using TF-IDF and storing it in a vector database.\n",
    "-   Vectorize our search query (`text`, created earlier) on the fly and retrieve its closest match from the vector database.\n",
    "\n",
    "Vectorizing our corpus is very straightforward using `Langchain`,\n",
    "as `Scikit`’s `TfidfVectorizer` is wrapped in a dedicated module provided by `Langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(spooky_df, page_content_column=\"text_clean\")\n",
    "\n",
    "retriever = TFIDFRetriever.from_documents(\n",
    "    loader.load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5363f-614b-49ae-8667-12242200fa43",
   "metadata": {},
   "source": [
    "This `retriever` object serves as an entry point to our corpus. `Langchain` is particularly valuable in NLP projects because it provides standardized entry points, allowing you to easily switch out vectorizers without needing to change how the results are used at the end of the pipeline.\n",
    "\n",
    "The `invoke` method is used to find the most similar vectors to our search query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656461c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'id12587', 'text': 'Listen to me, Frankenstein.', 'author': 'MWS', 'author_encoded': 2}, page_content='listen to me frankenstein'),\n",
       " Document(metadata={'id': 'id09284', 'text': 'I screamed aloud that I was not afraid; that I never could be afraid; and others screamed with me for solace.', 'author': 'HPL', 'author_encoded': 1}, page_content='i screamed aloud that i was not afraid that i never could be afraid and others screamed with me for solace'),\n",
       " Document(metadata={'id': 'id09797', 'text': 'It seemed to be a sort of monster, or symbol representing a monster, of a form which only a diseased fancy could conceive.', 'author': 'HPL', 'author_encoded': 1}, page_content='it seemed to be a sort of monster or symbol representing a monster of a form which only a diseased fancy could conceive'),\n",
       " Document(metadata={'id': 'id10816', 'text': 'And, as I have implied, it was not of the dead man himself that I became afraid.', 'author': 'HPL', 'author_encoded': 1}, page_content='and as i have implied it was not of the dead man himself that i became afraid')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779d59-7d64-4814-a2e8-4685e09a5db6",
   "metadata": {},
   "source": [
    "The output is a `Langchain` object, which is not convenient for our purposes here. We convert it into a *DataFrame*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for best_echoes in retriever.invoke(text):\n",
    "    documents += [{**best_echoes.metadata, **{\"text_clean\": best_echoes.page_content}}]\n",
    "\n",
    "documents = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714ca7a-5bb3-4254-95cc-d2ecdcd5d31d",
   "metadata": {},
   "source": [
    "We can add the similarity score column to this *DataFrame*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e623c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "documents['score'] = cosine_similarity(\n",
    "    pipeline_tfidf.transform(documents['text_clean']),\n",
    "    pipeline_tfidf.transform([text])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8ab21-4399-4f48-b006-d2bf4ece883e",
   "metadata": {},
   "source": [
    "We do indeed retrieve the same documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552fb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23b96a-22dd-4385-8e7e-beb91e4cb79a",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-note callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> The BM25 Metric\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> BM25 is a probabilistic relevance-based information retrieval model, similar to TF-IDF. It is commonly used in search engines to rank documents relative to a query.\n",
    ">\n",
    "> BM25 combines term frequency (TF), inverse document frequency (IDF), and a normalization based on document length. In other words, it improves on TF-IDF by adjusting scores based on string length to avoid overemphasizing longer documents.\n",
    ">\n",
    "> BM25 performs particularly well in environments where documents vary in length and content. This is why search engines such as `Elasticsearch` have made it a cornerstone of their search mechanisms.\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "Why aren’t all results relevant? We can anticipate several reasons.\n",
    "\n",
    "The first hypothesis is that we’re training our vectorizer on a biased corpus. While “Frankenstein” is a rare term, it appears more frequently in our dataset than in general English usage. The inverse document frequency is thus biased against the term: its appearance should be a much stronger indicator that the text belongs to Mary Shelley. While addressing this might slightly improve relevance, it’s not the core issue.\n",
    "\n",
    "The frequentist approach assumes all terms are equally distinct. A sentence containing the word *“creature”* won’t get a higher score when searching for *“monster”*. Again, we’ve treated our corpus as a bag where words are independent—there’s no increased likelihood of encountering *“Frankenstein”* after *“doctor”*. These limitations point us toward the topic of *embeddings*. Even though the frequentist method may seem a bit *old school*, it’s not useless and often provides a “tough to beat baseline.” In fields like information extraction from short texts, where every term carries strong signal, this approach is often effective.\n",
    "\n",
    "## 3.2 Finding the Closest Author: An Introduction to the Naive Bayes Classifier\n",
    "\n",
    "Before diving into *embeddings*, let’s explore a slightly different use case within our probabilistic framework. Suppose we want to predict the author of a given text. If our previous intuition holds—certain words are more likely to appear in texts by specific authors—then we can train an automatic classification algorithm to predict the author based on the text.\n",
    "\n",
    "The most natural method for this task is the Naive Bayes classifier. This model is a perfect fit for the frequentist approach we’ve used so far, as it relies on the probabilities of word occurrences per author.\n",
    "\n",
    "The Naive Bayes classifier applies a decision rule: it selects the most probable class given the observed structure of the document—i.e., the words that appear in it.\n",
    "\n",
    "In other words, we choose the class $\\widehat{c}$ that is most probable given the terms in document $d$.\n",
    "\n",
    "<span id=\"eq-definition-bayes\">$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(c|d) =  \\arg \\max_{c \\in \\mathcal{C}} \\frac{ \\mathbb{P}(d|c)\\mathbb{P}(c)}{\\mathbb{P}(d)}\n",
    " \\qquad(3.1)$$</span>\n",
    "\n",
    "As is common in Bayesian estimation, we can ignore constant terms such as $\\mathbb{P}(d)$. The definition of the predicted class can thus be reformulated as follows:\n",
    "\n",
    "<span id=\"eq-rewriting-bayes\">$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(d|c)\\mathbb{P}(c)\n",
    " \\qquad(3.2)$$</span>\n",
    "\n",
    "The bag-of-words assumption comes into play here. A document $d$ is considered a collection of words $w_i$, where word order is irrelevant. In other words, we can build a model based on individual words without involving conditional probabilities related to their order.\n",
    "The second strong assumption is the naive assumption from which the method gets its name: the probability of drawing a word depends only on the category $c$ to which the document belongs. In other words, a document is treated as a sequence of independent word draws, where the probability depends solely on the author.\n",
    "\n",
    "As explained in the dedicated box, under these assumptions, the classifier can be rewritten in the following form\n",
    "\n",
    "$$\n",
    "\\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(c)\\prod_{w \\in \\mathcal{W}}{\\mathbb{P}(w|c)}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{W}$ is the set of words in the corpus (our vocabulary).\n",
    "\n",
    "Empirically, this is a supervised learning task where the *label* is the document class and the *features* are our vectorized words. In practice, the probabilities are estimated from word counts in the corpus and the distribution of document types.\n",
    "\n",
    "While it is possible to compute all these quantities manually, `Scikit` makes it easy to implement a Naive Bayes estimator after vectorizing the corpus, as shown in the next exercise. However, this may introduce a practical issue: ideally, the test set should not contain new words that were not in the training set, since these new dimensions did not exist during training. In practice, the most common solution is the one adopted here: these words are ignored.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Exercise 2: The Naive Bayes Classifier\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> 1.  Starting from the previous example, define a *pipeline* that vectorizes each document (using `CountVectorizer` instead of `TFIDFVectorizer`) and performs prediction using a Naive Bayes model.\n",
    "> 2.  Train this model and make predictions on the test set.\n",
    "> 3.  Evaluate the performance of your model.\n",
    "> 4.  Make a prediction for the sentence we previously stored in the `text` variable. Do you get the expected result?\n",
    "> 5.  Examine the predicted probabilities (using the `predict_proba` method).\n",
    ">\n",
    "> </div>\n",
    "> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f9bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-3');</script></body>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b00f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e23e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7594781-4315-4252-a3eb-a458972e317c",
   "metadata": {},
   "source": [
    "We obtain a satisfactory accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 0.8429519918283963"
     ]
    }
   ],
   "source": [
    "print(f\"Précision: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7074f-8d9e-4d57-979c-1eeec3558cc8",
   "metadata": {},
   "source": [
    "The breakdown of performance metrics is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8815f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.85      0.85      0.85      1580\n",
      "         HPL       0.87      0.82      0.84      1127\n",
      "         MWS       0.81      0.86      0.83      1209\n",
      "\n",
      "    accuracy                           0.84      3916\n",
      "   macro avg       0.84      0.84      0.84      3916\n",
      "weighted avg       0.84      0.84      0.84      3916\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(y_test, y_pred)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befa88f-01ae-4600-8a95-fc940d2854de",
   "metadata": {},
   "source": [
    "Unsurprisingly, we get Mary Shelley as the predicted author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14461b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('MWS')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.predict([text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e530e5-7433-49cc-bf7e-288548432709",
   "metadata": {},
   "source": [
    "Finally, when examining the predicted probabilities (question 5), we see that the prediction is very confident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c4423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"author\": pipeline.classes_,\n",
    "        \"proba\": pipeline.predict_proba([text])[0]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273cb34-cf1a-42b2-85ae-964ce1d43508",
   "metadata": {},
   "source": [
    "> **Tip**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Understanding the logic of the naive Bayes classifier\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> Suppose we are in a classification problem with classes $(c_1,...,c_K)$ (set denoted $\\mathcal{C}$). Placing ourselves within the bag-of-words framework, we can ignore the positions of words in documents, which would greatly complicate the writing of our equations.\n",
    ">\n",
    "> The equation <a href=\"#eq-rewriting-bayes\" class=\"quarto-xref\">Equation 3.2</a> can be rewritten\n",
    ">\n",
    "> $$\n",
    "> \\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(w_1, ..., w_n|c)\\mathbb{P}(c)\n",
    "> $$\n",
    ">\n",
    "> In the Bayesian world, we call $\\mathbb{P}(w_1, ..., w_n|c)$ the likelihood and $\\mathbb{P}(c)$ the prior.\n",
    ">\n",
    "> The naive Bayes assumption allows us to treat a document as a sequence of random draws whose probabilities depend only on the category. In this case, drawing a sentence is a sequence of word draws and the compound probability is therefore\n",
    ">\n",
    "> $$\n",
    "> \\mathbb{P}(w_1, ..., w_n|c) = \\prod_{i=1}^n \\mathbb{P}(w_i|c)\n",
    "> $$\n",
    ">\n",
    "> For example, simplifying to two classes, if the probabilities are those from <a href=\"#tbl-fake-proba\" class=\"quarto-xref\">Table 3.1</a>, the sentence *“afraid by Doctor Frankenstein”* will have a little less than 1% chance (0.8%) of being written if the author is Mary Shelley but will be even less likely with Lovecraft (0.006%) because while *“afraid”* is very probable with him, Frankenstein is a rare event that makes this word composition unlikely.\n",
    ">\n",
    "> | Word ($w_i$) | Probability for Mary Shelley | Probability for Lovecraft |\n",
    "> |--------------|------------------------------|---------------------------|\n",
    "> | Afraid       | 0.1                          | 0.6                       |\n",
    "> | By           | 0.2                          | 0.2                       |\n",
    "> | Doctor       | 0.2                          | 0.05                      |\n",
    "> | Frankenstein | 0.2                          | 0.01                      |\n",
    ">\n",
    "> Table 3.1: Fictional example of drawing probabilities\n",
    ">\n",
    "> By combining these different equations, we get\n",
    ">\n",
    "> $$\n",
    "> \\widehat{c} = \\arg \\max_{c \\in \\mathcal{C}} \\mathbb{P}(c)\\prod_{w \\in \\mathcal{W}}{\\mathbb{P}(w|c)}\n",
    "> $$\n",
    ">\n",
    "> The empirical counterpart of $\\mathbb{P}(c)$ is quite obvious: the observed frequency of each category (the authors) in our corpus. In other words,\n",
    ">\n",
    "> $$\n",
    "> \\widehat{\\mathbb{P}(c)} = \\frac{n_c}{n_{doc}}\n",
    "> $$\n",
    ">\n",
    "> What is the empirical counterpart of $\\mathbb{P}(w_i|c)$? It is the frequency of appearance of the word in question for the author. To calculate it, we simply count the number of times it appears for the author and divide by the number of words by the author.\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "# 4. The Word2Vec model, a more synthetic representation\n",
    "\n",
    "## 4.1 Towards a more synthetic representation of language\n",
    "\n",
    "The vector representation resulting from the *bag of words* approach is not very synthetic or stable and is quite crude.\n",
    "\n",
    "If we have a small corpus, we will have problems extrapolating since new texts are very likely to bring new words, which are new feature dimensions that were not present in the training corpus. This is conceptually a problem since *machine learning* algorithms are not intended to predict on characteristics they have not been trained on[1].\n",
    "\n",
    "Conversely, the more text we have in a corpus, the larger our vector representation will be. For example, if your bag of words has seen the entire French vocabulary, which is 60,000 words according to the [French Academy](https://www.dictionnaire-academie.fr/article/QDL056) (estimates being 200,000 for the English language), this results in vectors of considerable size. However, the diversity of texts is, in practice, much lower: common use of French requires around 3,000 words and most texts, especially if they are short, do not use such a complete vocabulary. This therefore implies very sparse vectors, with many 0s.\n",
    "\n",
    "Vectorization according to this approach is therefore inefficient; the signal is poorly compressed. Dense representations, that is, of smaller dimension but all carrying information, seem more adequate to be able to generalize our language modeling.\n",
    "The algorithm that made this approach famous is the `Word2Vec` model, in some ways the first common ancestor of modern LLMs. The vector representation of `Word2Vec` is quite synthetic: the dimension of these *embeddings* is between 100 and 300.\n",
    "\n",
    "## 4.2 Semantic relationships between terms\n",
    "\n",
    "This dense representation will represent a solution to a limitation of the *bag of words* approach that we have mentioned multiple times. Each of these dimensions will represent a latent factor,\n",
    "that is, an unobserved variable,\n",
    "in the same way as principal components produced by a PCA. These latent dimensions can be interpreted as “fundamental” dimensions of language.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://jalammar.github.io/images/word2vec/word2vec.png\" alt=\"Illustration of the principle of Word2Vec representation (source: Jay Alammar)\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration of the principle of Word2Vec representation (source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\">Jay Alammar</a>)</figcaption>\n",
    "</figure>\n",
    "\n",
    "For example, a human knows that a document containing the word *“King”*\n",
    "and another document containing the word *“Queen”* are very likely\n",
    "to address similar subjects. A well-trained `Word2Vec` model will capture\n",
    "that there exists a latent factor of type *“royalty”*\n",
    "and the similarity between the vectors associated with the two words will be strong.\n",
    "\n",
    "The magic goes even further: the model will also capture that there exists a\n",
    "latent factor of type *“gender”*,\n",
    "and will allow the construction of a semantic space in which\n",
    "arithmetic relationships between vectors make sense. For example,\n",
    "\n",
    "$$\n",
    "\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\n",
    "$$\n",
    "\n",
    "or, to revisit the example from the original `Word2Vec` paper (Mikolov 2013),\n",
    "\n",
    "$$\n",
    "\\text{Paris} - \\text{France} + \\text{Italy} ≈ \\text{Rome}\n",
    "$$\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://ssphub.netlify.app/post/embedding/word_embedding.png\" alt=\"Illustration of lexical embedding. Source: Blog post Word Embedding: Basics\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration of lexical embedding. Source: Blog post <a href=\"https://medium.com/@hari4om/word-embedding-d816f643140\">Word Embedding: Basics</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Another “miracle” of this approach is that it allows a form of transfer between languages. Since semantic relationships can be similar across languages, many common words can be mapped between languages if they share a common base (such as Western languages). This concept is the foundation of automatic translators and multilingual AI systems.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://engineering.fb.com/wp-content/uploads/2018/01/GJ_9lgFMnVaR0ZYAAAAAAABV9MkQbj0JAAAC.gif\" alt=\"Example of translation between two vector representations. Source: Meta\" />\n",
    "<figcaption aria-hidden=\"true\">Example of translation between two vector representations. Source: <a href=\"https://engineering.fb.com/2018/01/24/ml-applications/under-the-hood-multilingual-embeddings/\">Meta</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "## 4.3 How are these models trained?\n",
    "\n",
    "These models are trained from a prediction task solved by a simple neural network, generally with a reinforcement approach.\n",
    "\n",
    "The fundamental idea is that the meaning of a word is understood by looking at words that frequently appear in its neighborhood. For a given word, we will therefore try to predict the words that appear in a window around the target word.\n",
    "\n",
    "By repeating this task many times and on a sufficiently varied corpus,\n",
    "we finally obtain *embeddings* for each word in the vocabulary,\n",
    "which present the properties discussed previously. The collection of `Wikipedia` articles is one of the preferred corpora for people who have built lexical\n",
    "embeddings. It indeed contains complete sentences, unlike information from social media comments,\n",
    "and proposes interesting connections between people, places, etc.\n",
    "\n",
    "The context of a word is defined by a fixed-size window around this word. The window size is a parameter of the *embedding* construction. The corpus provides a large set of word-context examples, which can be used to train a neural network.\n",
    "\n",
    "More precisely, there are two approaches, whose details we will not develop:\n",
    "\n",
    "-   *Continuous bag of words* (CBOW), where the model is trained to predict a word from its context;\n",
    "-   *Skip-gram*, where the model attempts to predict the context from a single word.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://ssphub.netlify.app/post/embedding/CBOW_Skipgram_training.png\" alt=\"Illustration of the difference between CBOW and Skip-gram approaches\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration of the difference between CBOW and Skip-gram approaches</figcaption>\n",
    "</figure>\n",
    "\n",
    "## 4.4 Related models\n",
    "\n",
    "Several models have a direct lineage with the `Word2Vec` model although they distinguish themselves by the nature of the architecture used.\n",
    "\n",
    "This is the case, for example, of the [`GloVe`](https://nlp.stanford.edu/projects/glove/) model, developed in 2014 at Stanford,\n",
    "which does not rely on neural networks but on the construction of a large word co-occurrence matrix. For each word, the task is to calculate the frequencies of appearance of other words in a fixed-size window around it. The obtained co-occurrence matrix is then factorized by a singular value decomposition.\n",
    "\n",
    "The [`FastText`](https://fasttext.cc/) model, developed in 2016 by a `Facebook` team, works similarly to `Word2Vec` but particularly distinguishes itself on two points:\n",
    "\n",
    "-   In addition to the words themselves, the model learns representations for character n-grams (character subsequences of size $n$, for example *“tar”*, *“art”* and *“rte”* are the trigrams of the word *“tarte”*), which makes it particularly robust to spelling variations;\n",
    "-   The model has been optimized so that its training is particularly fast.\n",
    "\n",
    "The [`FastText`](https://fasttext.cc/) model is particularly effective for automatic classification problems. INSEE uses it for example for several models of classification of textual labels in nomenclatures.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://ssphub.netlify.app/post/embedding/fasttext.png\" alt=\"Illustration of the fastText model\" />\n",
    "<figcaption aria-hidden=\"true\">Illustration of the fastText model</figcaption>\n",
    "</figure>\n",
    "\n",
    "Here is an example of an automated profession classification project in the typology\n",
    "of activity nomenclatures (PCS) based on a model trained by the `Fasttext` library:\n",
    "\n",
    "[1] This remark may seem surprising while generative AIs occupy an important place in our usage. Nevertheless, we must keep in mind that while you ask new questions to AIs, you ask them in terms they know: natural language in a language present in their training corpus, digital images that are therefore interpretable by a machine, etc. In other words, your *prompt* is not, in itself, unknown to the AI, it can interpret it even if its content is new and original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de l'appel API : 503 Server Error: Service Temporarily Unavailable for url: https://codification-ape-test.lab.sspcloud.fr/predict?nb_echos_max=3&prob_min=0&text_feature=data%20scientist"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "activite = \"data scientist\"\n",
    "urlApe = (\n",
    "    \"https://codification-ape-test.lab.sspcloud.fr/\"\n",
    "    f\"predict?nb_echos_max=3&prob_min=0&text_feature={activite}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # requête\n",
    "    resp = requests.get(urlApe, timeout=10)\n",
    "    resp.raise_for_status()  # lève une erreur si code HTTP != 200\n",
    "    data = resp.json()\n",
    "\n",
    "    # récupération de IC\n",
    "    IC = data.pop(\"IC\", None)\n",
    "\n",
    "    # transformation en DataFrame\n",
    "    df = pd.DataFrame(data.values())\n",
    "    df[\"indice_confiance\"] = IC\n",
    "\n",
    "    print(df)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Erreur lors de l'appel API :\", e)\n",
    "    df = pd.DataFrame()  # DataFrame vide en cas d'échec\n",
    "\n",
    "except (ValueError, KeyError) as e:\n",
    "    print(\"Erreur lors du parsing des données :\", e)\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a1bc0-87da-4c81-aa1a-61aebaad0d6d",
   "metadata": {},
   "source": [
    "These models are inheritors of `Word2Vec` in the sense that they adopt a dense, low-dimensional vector representation of textual documents. `Word2Vec` remains a model that inherits from the bag-of-words logic. The representation of a sentence or document is a form of average of the representations of the words that compose them.\n",
    "\n",
    "Since 2013, several revolutions have led to enriching language models to go beyond a word-by-word representation of these. Much more complex architectures to represent not only words in the form of *embeddings* but also sentences and documents are now at work and can be linked to the revolution of *transformer* architectures.\n",
    "\n",
    "# 5. *Transformers*: a richer representation of language\n",
    "\n",
    "While the `Word2Vec` model is trained contextually, its purpose is to give a vector representation of a word in an absolute manner, independent of context. For example, the term *“bank”* will have exactly the same vector representation whether it appears in the sentence *“She runs towards the sandbank”* or *“He’s waiting for you on a bench in the park”*. This is a major limitation of this type of approach and we can well imagine the importance of context for language interpretation.\n",
    "\n",
    "The objective of *transformer* architectures is to enable contextual vector representations. In other words, a word will have several vector representations, depending on its context of occurrence. These models rely on the attention mechanism (Vaswani 2017). Before this approach, when a model learned to vectorize a text and reached the nth word, the only memory it kept was that of the previous word. By recurrence, this meant it kept a memory of previous words but this tended to dissipate. Consequently, for a word appearing far in the sentence, it was likely that the context from the beginning of the sentence was forgotten. In other words, in the sentence *“at the beach, he was going to explore the bank”*, it was very likely that upon reaching the word *“bank”*, the model had forgotten the beginning of the sentence which was nevertheless important for interpretation.\n",
    "\n",
    "The objective of the attention mechanism is to create an internal memory in the model allowing, for any word in a text, to keep track of other words. Of course, not all are relevant for interpreting the text but this avoids forgetting those that are important. The main innovation of recent years in NLP has been to manage to create large-scale attention mechanisms without making the models intractable. The context windows of the most performant models are becoming immense. For example, the Llama 3.1 model (made public by Meta in July 2024) offers a context window of 128,000 *tokens*, or about 96,000 words, the equivalent of Tolkien’s *Hobbit*. In other words, to deduce the subtlety of a word’s meaning, this model can browse through a context as long as a novel of about 300 pages.\n",
    "\n",
    "The two models that marked their era in the field are the `BERT` models developed in 2018 by *Google* (which was already behind `Word2Vec`) and the first version of the well-known `GPT` from `OpenAI`, which, in 2017, was the first pre-trained model based on the *transformer* architecture. These two *transformer* families differ in how they integrate context to make a prediction. `GPT` is an autoregressive model, therefore only considers the *tokens* before the one we want to predict. `BERT` uses the *tokens* to the left and right to infer context. These two major trained language models are trained by self-reinforcement, mainly on next *token* prediction tasks (“The Hugging Face Course, 2022” 2022). Since the success of `ChatGPT`, the new GPT models (from version 3 onwards) are no longer *open source*. To use them, one must therefore go through OpenAI’s APIs. There are nevertheless many alternatives whose weights are open, if not *open source*[1], which allow using these LLMs through `Python`, notably through the `transformers` library developed by *Hugging Face*.\n",
    "\n",
    "When working with small-sized corpora,\n",
    "it’s generally a bad idea to train your own model *from scratch*. Fortunately, models pre-trained on very large corpora are available. They allow for *transfer learning*, that is, to benefit from the performance of a model that has been trained on another task or on another corpus.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> <div class=\"callout callout-style-default callout-tip callout-titled\">\n",
    "> <div class=\"callout-header d-flex align-content-center\">\n",
    "> <div class=\"callout-icon-container\">\n",
    "> <i class=\"callout-icon\"></i>\n",
    "> </div>\n",
    "> <div class=\"callout-title-container flex-fill\">\n",
    "> Exercise 3\n",
    "> </div>\n",
    "> </div>\n",
    "> <div class=\"callout-body-container callout-body\">\n",
    ">\n",
    "> 1.  Repeat a train/test split with 500 random lines\n",
    "> 2.  Import the `all-MiniLM-L6-v2` model with the `sentence transformers` package. Encode `X_train` and `X_test`.\n",
    "> 3.  Perform a classification using a simple method, such as CVS, based on the *embeddings* produced in the previous question. As the training set is small, you can perform cross-validation.\n",
    "> 4.  Understand why the performance is worse than that of Bayes’ naive classifier.\n",
    ">\n",
    "> </div>\n",
    "> </div>\n",
    "\n",
    "Answer to question 1:\n",
    "\n",
    "``` python\n",
    "random_rows = spooky_df.sample(500)\n",
    "y = random_rows[\"author\"]\n",
    "X = random_rows['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "Réponse à la question 2:\n",
    "\n",
    "``` python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", model_kwargs={\"torch_dtype\": \"float16\"}\n",
    ")\n",
    "\n",
    "X_train_vectors = model.encode(X_train.values)\n",
    "X_test_vectors = model.encode(X_test.values)\n",
    "```\n",
    "\n",
    "Answer to question 3:\n",
    "\n",
    "``` python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LinearSVC(max_iter=10000, C=0.1, dual=\"auto\")\n",
    "\n",
    "scores = cross_val_score(\n",
    "    clf, X_train_vectors, y_train,\n",
    "    cv=4, scoring='f1_micro', n_jobs=4\n",
    ")\n",
    "\n",
    "print(f\"CV scores {scores}\")\n",
    "print(f\"Mean F1 {np.mean(scores)}\")\n",
    "```\n",
    "\n",
    "**But why, with a very complicated method, can’t we beat a very simple one?**\n",
    "\n",
    "There are several possible reasons:\n",
    "\n",
    "-   the TF-IDF is a simple model, but it still performs very well\n",
    "    (this is known as a ‘tough-to-beat baseline’).\n",
    "-   the classification of authors is a very specific and arduous task,\n",
    "    which does not do justice to the *embeddings*. As we said earlier, the latter are particularly relevant when it comes to semantic similarity between texts (*clustering*, etc.).\n",
    "\n",
    "In the case of our classification task, it is likely that\n",
    "certain words (character names, place names) are sufficient to classify in a relevant way,\n",
    "This is not captured by *embeddings*, which give all words the same importance.\n",
    "\n",
    "Mikolov, Tomas. 2013. “Efficient Estimation of Word Representations in Vector Space.” *arXiv Preprint arXiv:1301.3781* 3781.\n",
    "\n",
    "“The Hugging Face Course, 2022.” 2022. <https://huggingface.co/course>.\n",
    "\n",
    "Vaswani, A. 2017. “Attention Is All You Need.” *Advances in Neural Information Processing Systems*.\n",
    "\n",
    "[1] Some organizations, like Meta for Llama, make available the post-training weights of their model on the *Hugging Face* platform, allowing reuse of these models if the license permits. Nevertheless, these are not *open source* models since the code used to train the models and constitute the learning corpora, derived from massive data collection by *webscraping*, and any additional annotations to make specialized versions, are not shared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/runner/work/python-datascientist/python-datascientist/.venv/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
